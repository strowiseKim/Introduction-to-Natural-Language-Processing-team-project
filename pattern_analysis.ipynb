{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96122f29-b9a6-44d1-b5a5-b9e7586205db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6dee7c3-3146-4401-84d9-4dbd94513155",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read = pd.read_json(\"hf://datasets/HumanLLMs/Human-Like-DPO-Dataset/data.json\")\n",
    "data = pd.read_csv('generated_essays_merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb229b03-1df4-4e55-b9fb-214840ad5093",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\strow\\AppData\\Local\\Programs\\Python\\Python313\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\strow\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\strow\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\strow\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\strow\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\strow\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\strow\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\strow\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "# í˜„ì¬ ì£¼í”¼í„° ì»¤ë„ì´ ì‚¬ìš©í•˜ëŠ” íŒŒì´ì¬ í™˜ê²½ì— pip ëª…ë ¹ì„ ì „ë‹¬\n",
    "!{sys.executable} -m pip install nltk\n",
    "import re\n",
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c026409-1a89-4fb8-a2d5-a6276ae8e8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ì²« ë²ˆì§¸ DataFrame (df_chosen)ì˜ íŠ¹ì§• ì¶”ì¶œ ê²°ê³¼ í™•ì¸ ---\n",
      "                                                text  text_length  word_count  \\\n",
      "0  ğŸ˜‚ Ah, no I haven't! I'm dying to know, what's ...          125          26   \n",
      "1  Oh, totally! ğŸ˜„ I'm a sucker for a good ol' roc...          316          64   \n",
      "2  ğŸ˜Š I'm actually a big fan of DIY projects! I'm ...          497          92   \n",
      "3  Oh, man! I'm a total sucker for Italian food! ...          624         113   \n",
      "4  You know, I've always been fascinated by music...          493          95   \n",
      "\n",
      "   avg_sentence_length  avg_word_length  complex_word_ratio  \n",
      "0             4.800000         3.541667            0.083333  \n",
      "1             7.500000         3.700000            0.116667  \n",
      "2             9.888889         4.134831            0.179775  \n",
      "3             9.818182         4.259259            0.175926  \n",
      "4            15.333333         3.978261            0.108696  \n"
     ]
    }
   ],
   "source": [
    "df_chosen = df_read[['chosen']]\n",
    "df_rejected = df_read[['rejected']]\n",
    "data_dpo = data[['HumanLLMs/Human-Like-LLama3-8B-Instruct_Generated_Essay']]\n",
    "data_cft = data[['meta-llama/Meta-Llama-3-8B-Instruct_Generated_Essay']]\n",
    "data_human = data[['Human_Written_Essay']]\n",
    "\n",
    "def extract_features(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "    words_clean = [w for w in words if w.isalnum()]\n",
    "    \n",
    "    features = {}\n",
    "\n",
    "    features['avg_sentence_length'] = len(words_clean) / max(len(sentences), 1)\n",
    "    features['avg_word_length'] = np.mean([len(w) for w in words_clean]) if words_clean else 0\n",
    "    \n",
    "    # Complex words (>6 characters)\n",
    "    complex_words = [w for w in words_clean if len(w) > 6]\n",
    "    features['complex_word_ratio'] = len(complex_words) / max(len(words_clean), 1)\n",
    "    \n",
    "    return features\n",
    "\n",
    "Wholedata = [df_chosen, df_rejected, data_dpo, data_cft, data_human]\n",
    "\n",
    "updated_data = []\n",
    "\n",
    "for i, df in enumerate(Wholedata):\n",
    "    df.columns = ['text']\n",
    "    \n",
    "    # ê¸°ë³¸ feature ì¶”ê°€\n",
    "    df['text_length'] = df['text'].str.len()\n",
    "    df['word_count'] = df['text'].str.split().str.len()\n",
    "    \n",
    "    # ì´ì™¸ì˜ feature ì¶”ì¶œ\n",
    "    features_series = df['text'].apply(extract_features)\n",
    "    features_df = pd.json_normalize(features_series)\n",
    "    \n",
    "    # ì›ë³¸ dfì— íŠ¹ì§•ì„ ì¶”ê°€í•œ í›„, ì—…ë°ì´íŠ¸ëœ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥\n",
    "    updated_df = pd.concat([df, features_df], axis=1)\n",
    "    updated_data.append(updated_df)\n",
    "    \n",
    "# ë°˜ë³µë¬¸ì´ ì™„ë£Œë˜ë©´ updated_data ë¦¬ìŠ¤íŠ¸ì— ëª¨ë“  íŠ¹ì§•ì´ ì¶”ê°€ëœ DataFrameë“¤ì´ ì €ì¥ë©ë‹ˆë‹¤.\n",
    "# ì˜ˆ: updated_data[0]ì´ df_chosenì— í•´ë‹¹í•©ë‹ˆë‹¤.\n",
    "\n",
    "print(\"--- ì²« ë²ˆì§¸ DataFrame (df_chosen)ì˜ íŠ¹ì§• ì¶”ì¶œ ê²°ê³¼ í™•ì¸ ---\")\n",
    "print(updated_data[0].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00bd8a03-00df-426d-a5c2-702648f9af33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  pronoun_ratio\n",
      "0  ğŸ˜‚ Ah, no I haven't! I'm dying to know, what's ...       0.115385\n",
      "1  Oh, totally! ğŸ˜„ I'm a sucker for a good ol' roc...       0.125000\n",
      "2  ğŸ˜Š I'm actually a big fan of DIY projects! I'm ...       0.152174\n",
      "3  Oh, man! I'm a total sucker for Italian food! ...       0.088496\n",
      "4  You know, I've always been fascinated by music...       0.136842\n",
      "                                                text  pronoun_ratio\n",
      "0  I'm an artificial intelligence language model,...       0.115385\n",
      "1  As a professional AI language model, I don't h...       0.106061\n",
      "2  Good day. As a continuously evolving artificia...       0.104000\n",
      "3  In accordance with my programming, I must emph...       0.077586\n",
      "4  Good day. As a professional AI language model,...       0.060000\n",
      "                                                text  pronoun_ratio\n",
      "0  The bar chart reveals striking changes in the ...       0.003891\n",
      "1  Rich countries often provide financial aid to ...       0.041667\n",
      "2  The graph reveals the fluctuating numbers of o...       0.029661\n",
      "3  Some countries have been investing heavily in ...       0.035714\n",
      "4  The pie charts reveal a striking transformatio...       0.028689\n",
      "                                                text  pronoun_ratio\n",
      "0  The bar chart provides an insightful look at t...       0.007874\n",
      "1  The notion that financial aid from rich countr...       0.032407\n",
      "2  The graph depicting the number of overseas vis...       0.007491\n",
      "3  The trend of investing in specialized faciliti...       0.015564\n",
      "4  The percentage of time spent on various activi...       0.012931\n",
      "                                                text  pronoun_ratio\n",
      "0  Between 1995 and 2010, a study was conducted r...       0.000000\n",
      "1  Poverty represents a worldwide crisis. It is t...       0.027132\n",
      "2  Information about the thousands of visits from...       0.012346\n",
      "3  Whether countries should only invest facilitie...       0.052083\n",
      "4  Information is presented in two pie charts whi...       0.012931\n"
     ]
    }
   ],
   "source": [
    "ALL_PRONOUNS = ['i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours', \n",
    "                'you', 'your', 'yours', \n",
    "                'he', 'him', 'his', 'she', 'her', 'hers', 'it', 'its', \n",
    "                'they', 'them', 'their', 'theirs', \n",
    "                'one', 'oneself', 'it\\'s', 'he\\'s', 'she\\'s', 'they\\'re'] \n",
    "\n",
    "def count_pronouns(text):\n",
    "        # ëŒ€ëª…ì‚¬ì˜ ì´ ê°œìˆ˜ ì„¸ê¸°\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower()) # ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„ë¦¬\n",
    "    \n",
    "    pronoun_count = 0\n",
    "    for word in words:\n",
    "        if word in ALL_PRONOUNS:\n",
    "            pronoun_count += 1\n",
    "    return pronoun_count\n",
    "\n",
    "for x in range(5):\n",
    "    df = updated_data[x]  # ì²« ë²ˆì§¸ DataFrameì„ ì„ íƒ\n",
    "    \n",
    "    # 1. 'pronoun_count' ì—´ ì¶”ê°€\n",
    "    #    apply ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ê° í…ìŠ¤íŠ¸ì— ëŒ€í•œ ëŒ€ëª…ì‚¬ ê°œìˆ˜ë¥¼ ê³„ì‚°\n",
    "    df['pronoun_count'] = df['text'].apply(count_pronouns)\n",
    "    \n",
    "    # 2. 'pronoun_ratio' ì—´ ê³„ì‚°\n",
    "    #    ì´ ëŒ€ëª…ì‚¬ ìˆ˜ / ì´ ë‹¨ì–´ ìˆ˜ (word_count)\n",
    "    #    ì´ ë‹¨ì–´ ìˆ˜(word_count)ê°€ 0ì¸ ê²½ìš°ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ 1ì„ ë”í•´ ë‚˜ëˆ„ê±°ë‚˜, whereë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "    df['pronoun_ratio'] = df['pronoun_count'] / df['word_count'].replace(0, 1)\n",
    "    \n",
    "    # ê²°ê³¼ í™•ì¸ (ì„ íƒ ì‚¬í•­)\n",
    "    print(df[['text', 'pronoun_ratio']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f09313c2-b9db-4dfa-b9e2-56c5ffcca97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEDGING_WORDS = [\n",
    "    'might', 'could', 'may', 'can', # ëª¨ë‹¬ ë™ì‚¬\n",
    "    'possible', 'possibly', 'likely', 'unlikely', 'perhaps', 'maybe', # ê°€ëŠ¥ì„±\n",
    "    'suggests', 'seems', 'appears', 'indicates', 'arguable', 'arguably' # ì£¼ì¥ ì™„í™”\n",
    "]\n",
    "\n",
    "def count_hedging_words(text):\n",
    "    # í—¤ì§• ë‹¨ì–´ ê°œìˆ˜ ì„¸ê¸°\n",
    "    text_lower = text.lower()\n",
    "    hedging_count = 0\n",
    "    \n",
    "    # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ë‹¨ì–´ë§Œ ì¹´ìš´íŠ¸\n",
    "    for word in HEDGING_WORDS:\n",
    "        # 'i think'ì™€ ê°™ì€ êµ¬ë¬¸ë„ í¬í•¨í•˜ì—¬ count\n",
    "        if ' ' in word:\n",
    "            hedging_count += text_lower.count(word)\n",
    "        else:\n",
    "            # ë‹¨ì–´ ê²½ê³„ë¥¼ ì‚¬ìš©í•˜ì—¬ 'might'ê°€ 'almighty' ê°™ì€ ë‹¨ì–´ì— í¬í•¨ë˜ì§€ ì•Šë„ë¡ í•¨\n",
    "            pattern = r'\\b' + re.escape(word) + r'\\b'\n",
    "            hedging_count += len(re.findall(pattern, text_lower))\n",
    "            \n",
    "    return hedging_count\n",
    "\n",
    "for x in range(5):\n",
    "    df = updated_data[x]  # ì²« ë²ˆì§¸ DataFrame (df_chosen)ì„ ì„ íƒ\n",
    "    \n",
    "    # í—¤ì§• ë‹¨ì–´ ê°œìˆ˜ë¥¼ ê³„ì‚°\n",
    "    df['hedging_count'] = df['text'].apply(count_hedging_words)\n",
    "    \n",
    "    # í—¤ì§• ë¹„ìœ¨ ê³„ì‚°\n",
    "    df['hedging_ratio'] = df['hedging_count'] / df['word_count'].replace(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6c63525-51ce-4ca0-a590-d851b636d4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\strow\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "NOUN_TAGS = ['NN', 'NNS', 'NNP', 'NNPS'] \n",
    "VERB_TAGS = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "def calculate_noun_verb_ratio(text):\n",
    "    # ë‹¨ì–´ ë™ì‚¬ ë¹„ìœ¨ ê³„ì‚°\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "    \n",
    "    noun_count = 0\n",
    "    verb_count = 0\n",
    "    \n",
    "    for word, tag in tagged_words:\n",
    "        if tag in NOUN_TAGS:\n",
    "            noun_count += 1\n",
    "        elif tag in VERB_TAGS:\n",
    "            verb_count += 1\n",
    "            \n",
    "    # Noun/Verb Ratio ê³„ì‚°\n",
    "    if verb_count == 0:\n",
    "        return noun_count \n",
    "        \n",
    "    return noun_count / verb_count\n",
    "\n",
    "for x in range(5):\n",
    "    df = updated_data[x] \n",
    "    \n",
    "    df['noun_verb_ratio'] = df['text'].apply(calculate_noun_verb_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "067eda7a-be87-451a-b440-d56324f34cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADJECTIVE_TAGS = ['JJ', 'JJR', 'JJS']\n",
    "NOUN_TAGS = ['NN', 'NNS', 'NNP', 'NNPS'] \n",
    "\n",
    "# í”íˆ ëª…ì‚¬í™”ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì ‘ë¯¸ì‚¬ ëª©ë¡\n",
    "NOMINALIZATION_SUFFIXES = [\n",
    "    'tion', 'sion', 'ment', 'ness', 'ity', \n",
    "    'ism', 'ance', 'ence', 'ship', 'hood', 'age', 'al'\n",
    "]\n",
    "\n",
    "def calculate_nominalization_ratio(text):\n",
    "    # ëª…ì‚¬í™” ì ‘ë¯¸ì‚¬ë¥¼ ê°€ì§„ ë‹¨ì–´ì˜ ë¹„ìœ¨ì„ ê³„ì‚°\n",
    "    words_lower = nltk.word_tokenize(text.lower())\n",
    "    nominal_count = 0\n",
    "    \n",
    "    for word in words_lower:\n",
    "        # ë‹¨ì–´ê°€ ì •ì˜ëœ ì ‘ë¯¸ì‚¬ ì¤‘ í•˜ë‚˜ë¡œ ëë‚˜ëŠ”ì§€ í™•ì¸\n",
    "        if any(word.endswith(suffix) for suffix in NOMINALIZATION_SUFFIXES):\n",
    "            nominal_count += 1\n",
    "            \n",
    "    total_words = len(words_lower)\n",
    "    if total_words == 0:\n",
    "        return 0\n",
    "        \n",
    "    return nominal_count / total_words\n",
    "\n",
    "\n",
    "for x in range(5):\n",
    "    df = updated_data[x] \n",
    "\n",
    "    df['nominalization_ratio'] = df['text'].apply(calculate_nominalization_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a65c5ab-f83e-407a-bea2-751241f3fcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ['chosen', 'rejected', 'dpo', 'cft', 'human']\n",
    "\n",
    "for x in range(5):\n",
    "    updated_data[x].drop(columns = ['text', 'text_length', 'word_count', 'hedging_count', 'pronoun_count']).to_csv('data_'+name[x]+'.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8cdb7f-77da-4168-ba9c-ea0b3729a52f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
